{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "myenv",
   "display_name": "myenv",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "d56d92bfe18e338fca9330cc11fbdc9ce2394a956871bac79070cba5d437aed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Student Answer -> Bert encoding\\\n",
    "Question -> BERT encoding ------.->cross attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    " Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
    " As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "BERT is conceptually simple and empirically powerful. \n",
    "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_ans = \"Sky is red\"\n",
    "question = \"What is the colour of the sky?\"\n",
    "reference_ans =\"Sky appears blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBertEncoding(paragraph):\n",
    "        sentences = paragraph.split('.')\n",
    "        bert_embedding = BertEmbedding()\n",
    "        result = bert_embedding(sentences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = getBertEncoding(student_ans)\n",
    "Q = getBertEncoding(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.3888, -0.0291,  0.1319,  ..., -0.1076,  0.6338,  0.1131],\n",
       "        [ 0.4132, -0.4602,  0.1156,  ..., -0.4592,  0.2432, -0.1767],\n",
       "        [-0.0073,  0.1035,  0.0836,  ..., -0.1410,  0.6221, -0.1113]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_values = K[0][1]\n",
    "Q_values = Q[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_values = np.array(K_values)\n",
    "Q_values = np.array(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.from_numpy(K_values)\n",
    "Q = torch.from_numpy(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attention = attention(Q,K,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.7137, 0.1191, 0.1672],\n",
       "        [0.0628, 0.8501, 0.0871],\n",
       "        [0.1534, 0.6523, 0.1942],\n",
       "        [0.2714, 0.3641, 0.3644],\n",
       "        [0.1581, 0.6745, 0.1674],\n",
       "        [0.1706, 0.6694, 0.1599],\n",
       "        [0.5924, 0.2356, 0.1720],\n",
       "        [0.1272, 0.2982, 0.5746]])"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "K_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self,inp_dim,hid_dim,dropout = 0.1):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_sim = hid_dim\n",
    "        self.hidden = nn.Linear(inp_dim,hid_dim)\n",
    "        self.output = nn.Linear(hid_dim,inp_dim)\n",
    "        self.relu = nn.ReLU\n",
    "        self.dropout = nn.Dropout\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,attentionBlock,feedForwardBlock,size,dropout):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.attentionBlock = attentionBlock\n",
    "        self.feedForwardBlock = feedForwardBlock\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),2)\n",
    "\n",
    "    def forward(self,Query,Value):\n",
    "        Query = self.attentionBlock(query = Query, key = Value, value = Value)\n",
    "        x = self.sublayer[0](Query, lambda x: self.attentionBlock(query=x, value=Value, key=Value))\n",
    "        return self.sublayer[1](x, self.feedForwardBlock)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModule(nn.Module):\n",
    "    def __init__(self, EncoderLayer,N):\n",
    "        super(EncoderModule,self).__init__()\n",
    "        self.layers = clones(EncoderLayer,N)\n",
    "        self.norm = LayerNorm(self.EncoderLayer.size\n",
    "    \n",
    "    def forward(self,Query,Value):\n",
    "        for layers in self.layers:\n",
    "            Query = layers(Query = Query,Value = Value)\n",
    "        return Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationModule(nn.Module):\n",
    "    def __init__(self,EncoderModule):\n",
    "        super(RepresentationModule,self).__init__()\n",
    "        self.EncoderModules = clones(EncoderModule,2)\n",
    "        #Get 2 clones of EncoderModule\n",
    "    def forward(self, Question, ReferenceAnswer, StudentAnswer):\n",
    "        studentAnsRep = EncoderModules[0](Question,StudentAnswer)\n",
    "        RefAnsRep = EncoderModules[1](Question,ReferenceAnswer)\n",
    "        return concat(studentAnsRep,RefAnsRep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim = 768, num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value = K.reshape(3,1,768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = Q.reshape(8,1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.5666,  0.0905, -0.3259,  ...,  0.1991,  0.3866,  0.0199]],\n",
       "\n",
       "        [[ 0.2259,  0.1880,  0.2643,  ..., -0.1996,  0.0378,  0.6731]],\n",
       "\n",
       "        [[ 0.4941, -0.0662,  0.1847,  ...,  0.0330, -0.1765,  0.4608]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4937,  0.1258,  0.0544,  ..., -0.1227,  0.0570, -0.1065]],\n",
       "\n",
       "        [[ 0.8392,  0.2743, -0.0246,  ...,  0.1315,  0.3748, -0.1369]],\n",
       "\n",
       "        [[-0.0101,  0.0074, -0.9819,  ...,  0.0519,  0.2797, -0.2009]]])"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,att =multihead_attn(key =Value,value = Value,query = Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.0201,  0.0883,  0.0876,  ...,  0.2936,  0.1124, -0.2299]],\n",
       "\n",
       "        [[ 0.0235,  0.0832,  0.0853,  ...,  0.3003,  0.1076, -0.2377]],\n",
       "\n",
       "        [[ 0.0253,  0.0837,  0.0876,  ...,  0.3009,  0.1112, -0.2433]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0266,  0.0802,  0.0843,  ...,  0.3021,  0.1068, -0.2465]],\n",
       "\n",
       "        [[ 0.0245,  0.0842,  0.0832,  ...,  0.2978,  0.1063, -0.2371]],\n",
       "\n",
       "        [[ 0.0246,  0.0870,  0.0877,  ...,  0.3002,  0.1140, -0.2309]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mxnet 1.4.0 requires requests<2.19.0,>=2.18.4, but you have requests 2.25.1 which is incompatible.\n",
      "Collecting pytorch-pretrained-bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: requests in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from pytorch-pretrained-bert) (2.18.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from pytorch-pretrained-bert) (1.8.1+cpu)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.17.97-py2.py3-none-any.whl (131 kB)\n",
      "Requirement already satisfied: numpy in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from pytorch-pretrained-bert) (1.14.6)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.61.1-py2.py3-none-any.whl (75 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
      "Collecting botocore<1.21.0,>=1.20.97\n",
      "  Downloading botocore-1.20.97-py2.py3-none-any.whl (7.6 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from botocore<1.21.0,>=1.20.97->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.97->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\adi\\work\\clg\\capstone\\capstone\\venv\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Collecting requests\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tqdm, requests, regex, boto3, pytorch-pretrained-bert\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "Successfully installed boto3-1.17.97 botocore-1.20.97 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 regex-2021.4.4 requests-2.25.1 s3transfer-0.4.2 tqdm-4.61.1 urllib3-1.26.5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    }
   ],
   "source": [
    "%run BERT_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name '_multiarray_umath' from 'numpy.core' (d:\\Adi\\work\\CLG\\Capstone\\Capstone\\venv\\lib\\site-packages\\numpy\\core\\__init__.py)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5d2dede0532b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_multiarray_umath' from 'numpy.core' (d:\\Adi\\work\\CLG\\Capstone\\Capstone\\venv\\lib\\site-packages\\numpy\\core\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from numpy.core import _multiarray_umath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}